# Hidden Physics Models

We introduce [Hidden Physics Models](https://www.sciencedirect.com/science/article/pii/S0021999117309014), which are essentially data-efficient learning machines capable of leveraging the underlying laws of physics, expressed by time dependent and nonlinear [partial differential equations](https://en.wikipedia.org/wiki/Partial_differential_equation), to extract patterns from high-dimensional data generated from experiments. The proposed methodology may be applied to the problem of learning, system identification, or [data-driven discovery of partial differential equations](http://advances.sciencemag.org/content/3/4/e1602614.full). Our framework relies on [Gaussian Processes](http://www.gaussianprocess.org/gpml/), a powerful tool for probabilistic inference over functions, that enables us to strike a balance between model complexity and data fitting. The effectiveness of the proposed approach is demonstrated through a variety of canonical problems, spanning a number of scientific domains, including the [Navier-Stokes](https://en.wikipedia.org/wiki/Navier–Stokes_existence_and_smoothness), [Schrödinger](https://en.wikipedia.org/wiki/Nonlinear_Schrödinger_equation), [Kuramoto-Sivashinsky](https://www.encyclopediaofmath.org/index.php/Kuramoto-Sivashinsky_equation), and time dependent linear [fractional](https://en.wikipedia.org/wiki/Fractional_calculus) equations. The methodology provides a promising new direction for harnessing the long-standing developments of classical methods in applied mathematics and mathematical physics to design learning machines with the ability to operate in complex domains without requiring large quantities of data.

* * * * *
## Citation

	@article{raissi2017hidden,
	  title={Hidden physics models: Machine learning of nonlinear partial differential equations},
	  author={Raissi, Maziar and Karniadakis, George Em},
	  journal={Journal of Computational Physics},
	  year={2017},
	  publisher={Elsevier}
	}

